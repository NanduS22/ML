{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca89028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3836630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/student/NANDU DS\r\n"
     ]
    }
   ],
   "source": [
    "! pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbd6bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Brain Tumor', 829)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coubt the number of images 0 - cancer 1 - healthy\n",
    "number_of_images={}\n",
    "BASE_DIR = os.path.join(os.getcwd(),\"BRAIN tumor\",\"Brain Tumor\")\n",
    "for dir in os.listdir(BASE_DIR):\n",
    "    number_of_images[dir]=len(os.listdir(os.path.join(BASE_DIR, dir)))\n",
    "\n",
    "number_of_images.items() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fd16ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37336570",
   "metadata": {},
   "source": [
    "#we will split the data\n",
    "* 70% for training\n",
    "* 15% for validation\n",
    "* 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc8919d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder exist\n"
     ]
    }
   ],
   "source": [
    "# we will create a train folder\n",
    "\n",
    "if not os.path.exists(\"./train\"):\n",
    "    os.mkdir(\"./train\")\n",
    "    \n",
    "    for dir in os.listdir(BASE_DIR):\n",
    "        os.makedirs(\"./train\"+dir)\n",
    "        \n",
    "        for img in np.random.choice(a=os.listdir(os.path.join(BASE_DIR, dir)),  \n",
    "                                    size=(math.floor(70/100*number_of_images[dir])-5), replace=False):\n",
    "            O = os.path.join(BASE_DIR,dir,img)\n",
    "            D = os.path.join(\".\\train\",dir)\n",
    "            os.remove(O)\n",
    "else:\n",
    "    print(\"The folder exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b752db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataFolder(p, split):\n",
    "    \n",
    "    #we create training folder\n",
    "    \n",
    "    if not os.path.exists(\"./\"+p):\n",
    "        os.mkdir(\"./\"+p)\n",
    "    \n",
    "    for dir in os.listdir(BASE_DIR):\n",
    "        os.makedirs(\"./\"+p+\"/\"+dir)\n",
    "        \n",
    "        for img in np.random.choice(a=os.listdir(os.path.join(BASE_DIR, dir)),  \n",
    "                                    size=(math.floor(split*number_of_images[dir])-5), replace=False):\n",
    "            O = os.path.join(BASE_DIR,dir,img)\n",
    "            D = os.path.join(\"./\"+p,dir)\n",
    "            shutil.copy(O,D)\n",
    "            os.remove(O)\n",
    "    else:\n",
    "        print(f\"{p}The folder exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "629ea969",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './train/Brain Tumor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mdataFolder\u001b[0;34m(p, split)\u001b[0m\n\u001b[1;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mp)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(BASE_DIR):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(a\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;28mdir\u001b[39m)),  \n\u001b[1;32m     12\u001b[0m                                 size\u001b[38;5;241m=\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(split\u001b[38;5;241m*\u001b[39mnumber_of_images[\u001b[38;5;28mdir\u001b[39m])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m         O \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR,\u001b[38;5;28mdir\u001b[39m,img)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './train/Brain Tumor'"
     ]
    }
   ],
   "source": [
    "dataFolder(\"train\", 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661412fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder(\"val\", 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder(\"test\", 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in os.listdir(BASE_DIR):\n",
    "    number_of_images[dir]=len(os.listdir(os.path.join(BASE_DIR, dir)))\n",
    "\n",
    "number_of_images.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a38a9",
   "metadata": {},
   "source": [
    "# model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2cf365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 15:11:25.570776: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 15:11:25.673923: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 15:11:25.673935: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-19 15:11:25.691408: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-19 15:11:26.153143: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-19 15:11:26.153181: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-19 15:11:26.153186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAvgPool2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06d0671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 16)      448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 220, 220, 36)      5220      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 110, 110, 36)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 108, 108, 64)      20800     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 26, 26, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                5537856   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,638,245\n",
      "Trainable params: 5,638,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 15:11:28.052149: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-19 15:11:28.052167: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-19 15:11:28.052180: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tech-ThinkCentre-M80t): /proc/driver/nvidia/version does not exist\n",
      "2023-01-19 15:11:28.052308: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# CNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation=\"relu\", input_shape=(224,224,3)))\n",
    "\n",
    "model.add(Conv2D(filters=36, kernel_size=(3,3), activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86400754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=keras.losses.binary_crossentropy, metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8eb038",
   "metadata": {},
   "source": [
    "# #preparing our data using  data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7d6eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingImages1(path):\n",
    "    '''\n",
    "    input: path\n",
    "    OUTPUT: PRE PROCESSSED IMAGES\n",
    "    '''\n",
    "    image_data=ImageDataGenerator(zoom_range=0.2, shear_range=0.2, rescale=1/255, horizontal_flip=True)\n",
    "    image=image_data.flow_from_directory(directory=path, target_size=(224,224), batch_size=32, class_mode=\"binary\")\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1477289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2628 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "path=\"/home/student/NANDU DS/train\"\n",
    "train_data=preprocessingImages1(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5021855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingImages2(path):\n",
    "    '''\n",
    "    input: path\n",
    "    OUTPUT: PRE PROCESSSED IMAGES\n",
    "    '''\n",
    "    image_data=ImageDataGenerator(rescale=1/255 )\n",
    "    image=image_data.flow_from_directory(directory=path, target_size=(224,224), batch_size=32, class_mode=\"binary\")\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5355026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "path=\"/home/student/NANDU DS/test/\"\n",
    "test_data=preprocessingImages2(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1012fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 165 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "path=\"/home/student/NANDU DS/val/\"\n",
    "val_data=preprocessingImages2(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b56f294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping and model check point\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#early stopping\n",
    "\n",
    "es=EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "#model check point\n",
    "mc=ModelCheckpoint(monitor=\"val_accuracy\", filepath=\"./bestmodel.h5\", verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "cd=[es,mc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b21be",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8b0ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to ./bestmodel.h5\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "hs=model.fit_generator(generator= train_data, \n",
    "                       steps_per_epoch=8, \n",
    "                       epochs=30, verbose=1,\n",
    "                       validation_data=val_data, \n",
    "                       validation_steps=16, \n",
    "                       callbacks=cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486873a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model graphical interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7145ceba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=hs.history\n",
    "h.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d33ec97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(h['accuracy'])\n",
    "plt.plot(h['val_accuracy'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826bdc32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e54d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556608e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
